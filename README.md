# RAG LangChain OpenAI del proyecto

```
rag_langchain_openai/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ vectorstore.py        
â”‚   â”œâ”€â”€ splitter.py           
â”‚   â”œâ”€â”€ loader.py             
â”‚   â”œâ”€â”€ rag_agent.py          
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.py             
â”‚
â”œâ”€â”€ app.py                    
â”œâ”€â”€ ingest.py                 
â”œâ”€â”€ .env                      
â””â”€â”€ README.md                 
```
## âš™ï¸ ConfiguraciÃ³n inicial

Tener un entorno virtual activo y ejecutar:
```
pip install -r requirements.txt
```

Paquetes principales:

- langchain

- langchain_openai

- langchain_pinecone

- langchain_text_splitters

- pinecone-client

- python-dotenv

## ğŸ”‘ Configurar claves en ```.env```
```
OPENAI_API_KEY=tu_api_key_de_openai
PINECONE_API_KEY=tu_api_key_de_pinecone
PINECONE_ENVIRONMENT=us-east-1
PINECONE_INDEX_NAME=rag-index
```

## ğŸ§© ExplicaciÃ³n de los mÃ³dulos
### ğŸ§  1. ConexiÃ³n con Pinecone + OpenAI Embeddings

Esto se puede ver en el mÃ³dulo vectorstore.py, en este mÃ³dulo se usa OpenAIEmbeddings con el modelo text-embedding-3-small para generar representaciones vectoriales de texto.
Se conecta a Pinecone mediante su SDK y se crea un Ã­ndice serverless si no existe. Devuelve un PineconeVectorStore que LangChain utiliza para almacenar y buscar los embeddings.


## âœ‚ï¸ 2. DivisiÃ³n de documentos

Esto se puede ver en splitter.py, en este mÃ³dulo se usa RecursiveCharacterTextSplitter para dividir los textos en chunks manejables.

ParÃ¡metros configurables desde config.py:

- CHUNK_SIZE: tamaÃ±o de cada fragmento.

- CHUNK_OVERLAP: nÃºmero de caracteres de solapamiento.

Esto es importante porque los modelos de lenguaje trabajan mejor con fragmentos mÃ¡s pequeÃ±os de texto.

## ğŸ“š 3. Carga de documentos
Esto se puede ver en loader.py, donde se utiliza DirectoryLoader y TextLoader para leer automÃ¡ticamente todos los archivos .txt en una carpeta.

Devuelve una lista de Document de LangChain, lista para procesarse.

## ğŸ” 4. CreaciÃ³n del Agente RAG
Esto se puede ver en rag_agent.py creando un agente que combina recuperaciÃ³n de contexto + generaciÃ³n de respuesta:

Recupera los documentos mÃ¡s relevantes de Pinecone.

Genera respuestas con ChatOpenAI (gpt-4o-mini).

Usa RetrievalQA.from_chain_type de LangChain para unir el retriever (buscador) y el LLM (modelo generador).


## ğŸ’¾ 5. Variables globales
Esto se puede ver en config.py, donde se entralizan todas las configuraciones del proyecto:

Claves de OpenAI y Pinecone.

ParÃ¡metros de chunking.

Cantidad de resultados (TOP_K) recuperados en cada consulta.

## ğŸ’¬ 6. Interfaz CLI (Consola)
Se puede ver en app.py, este nos permite hacer preguntas en consola:

```
python app.py
```

El usuario escribe una pregunta y el agente RAG busca en la base vectorial la informaciÃ³n relevante y responde.

TambiÃ©n muestra las fuentes de donde provino la respuesta.

## ğŸ“¥ 7. Proceso de Ingesta
Se encuentra en ingest.py, el cual es un script que:

Carga los documentos con loader.py.

Los divide con splitter.py.

Los convierte en embeddings.

Los sube al Ã­ndice de Pinecone.

Se ejecuta una sola vez (o cada vez que se agregan documentos nuevos):

```
python ingest.py
```

## ğŸš€ EjecuciÃ³n completa
1. Ingestar los documentos
```
python ingest.py
```

2. Ejecutar el agente y hacer preguntas:
```
python app.py
```

3. Ejemplo de uso
```
ğŸš€ Bienvenido al RAG con LangChain + Pinecone + OpenAI
Escribe tu pregunta (o 'salir'): Â¿QuiÃ©n acuÃ±Ã³ el tÃ©rmino inteligencia artificial?

[Respuesta]
John McCarthy acuÃ±Ã³ el tÃ©rmino "inteligencia artificial" en 1956 durante la Conferencia de Dartmouth.

[Fuentes]
- data/ia_historia.txt
```
# Prueba del funcionamiento

- Prueba de que funciona la ingesta de datos
![alt text](images/image.png)
- Prueba de que funciona las preguntas segÃºn la informaciÃ³n ingestada
![alt text](images/image-1.png)

ğŸ§© TecnologÃ­as principales

- LangChain â€“ Framework de orquestaciÃ³n de LLMs.

- OpenAI API â€“ Modelos de lenguaje (gpt-4o-mini, text-embedding-3-small).

- Pinecone â€“ Almacenamiento y recuperaciÃ³n vectorial.

- Python â€“ ImplementaciÃ³n del flujo completo RAG.